{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc29677b",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1937f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c384171b",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda1784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)\n",
    "def Soup(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    heading_tags = [\"h1\", \"h2\", \"h3\"]  # there are only 3 headers.\n",
    "    for tags in soup.find_all(heading_tags):      # alternative method : soup.find_all(re.compile('^h[1-6]$'))\n",
    "        print (tags.name + ' -> ' + tags.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a9b9ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 -> Main Page\n",
      "h1 -> Welcome to Wikipedia\n",
      "h2 -> From today's featured article\n",
      "h2 -> Did you knowÂ ...\n",
      "h2 -> In the news\n",
      "h2 -> On this day\n",
      "h2 -> Today's featured picture\n",
      "h2 -> Other areas of Wikipedia\n",
      "h2 -> Wikipedia's sister projects\n",
      "h2 -> Wikipedia languages\n",
      "h2 -> Navigation menu\n",
      "h3 -> Personal tools\n",
      "h3 -> Namespaces\n",
      "h3 -> Views\n",
      "h3 -> Navigation\n",
      "h3 -> Contribute\n",
      "h3 -> Tools\n",
      "h3 -> Print/export\n",
      "h3 -> In other projects\n",
      "h3 -> Languages\n"
     ]
    }
   ],
   "source": [
    "Soup('https://en.wikipedia.org/wiki/Main_Page')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e66691e",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b96c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2)\n",
    "def Soup_100(url):\n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    names=[]\n",
    "    ratings=[]\n",
    "    years=[]\n",
    "    \n",
    "    for i in range(0,50):\n",
    "        \n",
    "            \n",
    "        common=soup.find_all('h3', class_='lister-item-header')\n",
    "        name=common[i].find('a').get_text()\n",
    "        names.append(name)\n",
    "        year=common[i].find('span', attrs={'class':'lister-item-year text-muted unbold'}).get_text()[1:-1]\n",
    "        years.append(year)\n",
    "        r=soup.find_all('div',attrs={'class':'inline-block ratings-imdb-rating'})\n",
    "        rate=float(r[i].find('strong').get_text())\n",
    "        ratings.append(rate)\n",
    "            \n",
    "    page=requests.get(url+'&start=51&ref_=adv_nxt')\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    for i in range(0,50):\n",
    "        \n",
    "            \n",
    "        common=soup.find_all('h3', class_='lister-item-header')\n",
    "        name=common[i].find('a').get_text()\n",
    "        names.append(name)\n",
    "        year=common[i].find('span', attrs={'class':'lister-item-year text-muted unbold'}).get_text()[1:-1]\n",
    "        years.append(year)\n",
    "        r=soup.find_all('div',attrs={'class':'inline-block ratings-imdb-rating'})\n",
    "        rate=float(r[i].find('strong').get_text())\n",
    "        ratings.append(rate)\n",
    "\n",
    "    \n",
    "\n",
    "    with open(\"top100_movies.csv\", 'w', newline='') as f:\n",
    "        \n",
    "        writer = csv.DictWriter(f, fieldnames=['Name', 'Rating', 'Year of release'])\n",
    "        writer.writeheader()\n",
    "    \n",
    "        for n,r,y in zip(names, ratings, years):\n",
    "            writer.writerow({'Name':n, 'Rating':r, 'Year of release':y})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3daaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Soup_100('https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd60d69",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6892825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) \n",
    "\n",
    "def Soup_100_indian(url):\n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    names=[]\n",
    "    ratings=[]\n",
    "    years=[]\n",
    "    \n",
    "    for i in range(0,100):\n",
    "        \n",
    "            \n",
    "        common=soup.find_all('td',class_='titleColumn')\n",
    "        name=common[i].find('a').get_text()\n",
    "        names.append(name)\n",
    "        year=common[i].find('span',class_='secondaryInfo').get_text()[1:-1]\n",
    "        years.append(year)\n",
    "        r=soup.find_all('td',class_='ratingColumn imdbRating')\n",
    "        rate=float(r[i].find('strong').get_text())\n",
    "        ratings.append(rate)\n",
    "        \n",
    "        \n",
    "    with open(\"top100_IndianMovies.csv\", 'w', newline='') as f:\n",
    "        \n",
    "        writer = csv.DictWriter(f, fieldnames=['Name', 'Rating', 'Year of release'])\n",
    "        writer.writeheader()\n",
    "    \n",
    "        for n,r,y in zip(names, ratings, years):\n",
    "            writer.writerow({'Name':n, 'Rating':r, 'Year of release':y})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "716ec2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Soup_100_indian('https://www.imdb.com/india/top-rated-indian-movies/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750eb139",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4973132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4)\n",
    "\n",
    "def Soup_presidents(url):\n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    names=[]\n",
    "    years=[]\n",
    "    \n",
    "    for i in range(14):\n",
    "        \n",
    "            \n",
    "        common=soup.find_all('div',class_='presidentListing')\n",
    "        x=common[i].find('h3').get_text().index('(')\n",
    "        name=common[i].find('h3').get_text()[:x]\n",
    "        names.append(name)\n",
    "        year=common[i].find('p').get_text()[16:]\n",
    "        years.append(year)\n",
    "    return (names,years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df3fa682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Shri Ram Nath Kovind ',\n",
       "  'Shri Pranab Mukherjee ',\n",
       "  'Smt Pratibha Devisingh Patil ',\n",
       "  'DR. A.P.J. Abdul Kalam ',\n",
       "  'Shri K. R. Narayanan ',\n",
       "  'Dr Shankar Dayal Sharma ',\n",
       "  'Shri R Venkataraman ',\n",
       "  'Giani Zail Singh ',\n",
       "  'Shri Neelam Sanjiva Reddy ',\n",
       "  'Dr. Fakhruddin Ali Ahmed ',\n",
       "  'Shri Varahagiri Venkata Giri ',\n",
       "  'Dr. Zakir Husain ',\n",
       "  'Dr. Sarvepalli Radhakrishnan ',\n",
       "  'Dr. Rajendra Prasad '],\n",
       " ['25 July, 2017 to 25 July, 2022 ',\n",
       "  '25 July, 2012 to 25 July, 2017 ',\n",
       "  '25 July, 2007 to 25 July, 2012 ',\n",
       "  '25 July, 2002 to 25 July, 2007 ',\n",
       "  '25 July, 1997 to 25 July, 2002 ',\n",
       "  '25 July, 1992 to 25 July, 1997 ',\n",
       "  '25 July, 1987 to 25 July, 1992 ',\n",
       "  '25 July, 1982 to 25 July, 1987 ',\n",
       "  '25 July, 1977 to 25 July, 1982 ',\n",
       "  '24 August, 1974 to 11 February, 1977',\n",
       "  '3 May, 1969 to 20 July, 1969 and 24 August, 1969 to 24 August, 1974',\n",
       "  '13 May, 1967 to 3 May, 1969',\n",
       "  '13 May, 1962 to 13 May, 1967',\n",
       "  '26 January, 1950 to 13 May, 1962'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Soup_presidents('https://presidentofindia.nic.in/former-presidents.htm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745b019",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b68faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5a)\n",
    "\n",
    "def Soup_cricket_top10_teams(url):\n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    teams=[]\n",
    "    matches=[]\n",
    "    points=[]\n",
    "    ratings=[]\n",
    "    \n",
    "    c=soup.find('tr', class_='rankings-block__banner')\n",
    "    team=c.find_all('td')[1].find('span',class_='u-hide-phablet').get_text()\n",
    "    teams.append(team)\n",
    "    match=int(c.find_all('td')[2].get_text())\n",
    "    matches.append(match)\n",
    "    point=c.find_all('td')[3].get_text()\n",
    "    points.append(point)\n",
    "    rate=int(c.find_all('td')[4].get_text().strip())\n",
    "    ratings.append(rate)\n",
    "    \n",
    "    for i in range(9):\n",
    "        \n",
    "        common=soup.find_all('tr',class_='table-body')\n",
    "        team=common[i].find('span', class_='u-hide-phablet').get_text()\n",
    "        teams.append(team)\n",
    "        match=int(common[i].find_all('td')[2].get_text())\n",
    "        matches.append(match)\n",
    "        point=common[i].find_all('td')[3].get_text()\n",
    "        points.append(point)\n",
    "        rate=int(common[i].find_all('td')[4].get_text())\n",
    "        ratings.append(rate)\n",
    "    return (teams,matches,points,ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39f115d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['New Zealand',\n",
       "  'England',\n",
       "  'Australia',\n",
       "  'India',\n",
       "  'Pakistan',\n",
       "  'South Africa',\n",
       "  'Bangladesh',\n",
       "  'Sri Lanka',\n",
       "  'Afghanistan',\n",
       "  'West Indies'],\n",
       " [23, 30, 32, 35, 22, 24, 30, 30, 19, 41],\n",
       " ['2,670',\n",
       "  '3,400',\n",
       "  '3,572',\n",
       "  '3,866',\n",
       "  '2,354',\n",
       "  '2,392',\n",
       "  '2,753',\n",
       "  '2,677',\n",
       "  '1,380',\n",
       "  '2,902'],\n",
       " [116, 113, 112, 110, 107, 100, 92, 89, 73, 71])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Soup_cricket_top10_teams('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f96c5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5b)\n",
    "\n",
    "def Soup_cricket_top10_batsmen(url):\n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    names=[]\n",
    "    teams=[]\n",
    "    ratings=[]\n",
    "    \n",
    "    c=soup.find('tr', class_='rankings-block__banner')\n",
    "    name=c.find('div',class_='rankings-block__banner--name-large').get_text()\n",
    "    names.append(name)\n",
    "    team=c.find('div' ,class_='rankings-block__banner--nationality').get_text().strip()\n",
    "    teams.append(team)\n",
    "    rate=c.find('div', class_='rankings-block__banner--rating').get_text()\n",
    "    ratings.append(rate)\n",
    "    \n",
    "    for i in range(9):\n",
    "        \n",
    "        common=soup.find_all('tr',class_='table-body')\n",
    "        name=common[i].find('a').get_text()\n",
    "        names.append(name)\n",
    "        team=common[i].find_all('td')[2].get_text().strip()\n",
    "        teams.append(team)\n",
    "        rate=int(common[i].find_all('td')[3].get_text())\n",
    "        ratings.append(rate)\n",
    "    return (names,teams,ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51f8aab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Babar Azam',\n",
       "  'Imam-ul-Haq',\n",
       "  'Rassie van der Dussen',\n",
       "  'Quinton de Kock',\n",
       "  'David Warner',\n",
       "  'Steve Smith',\n",
       "  'Jonny Bairstow',\n",
       "  'Virat Kohli',\n",
       "  'Rohit Sharma',\n",
       "  'Kane Williamson'],\n",
       " ['PAK', 'PAK', 'SA', 'SA', 'AUS', 'AUS', 'ENG', 'IND', 'IND', 'NZ'],\n",
       " ['890', 779, 766, 759, 747, 719, 710, 707, 704, 701])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Soup_cricket_top10_batsmen('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebe6a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5c)\n",
    "\n",
    "def Soup_cricket_top10_bowlers(url):\n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    names=[]\n",
    "    teams=[]\n",
    "    ratings=[]\n",
    "    \n",
    "    c=soup.find_all('div',class_='col-4 col-12-desk touch-scroll-list__element')\n",
    "    name=c[1].find('div',class_='rankings-block__banner--name').get_text()\n",
    "    names.append(name)\n",
    "    team=c[1].find('div' ,class_='rankings-block__banner--nationality').get_text().split('\\n')[2]\n",
    "    teams.append(team)\n",
    "    rate=c[1].find('div', class_='rankings-block__banner--rating').get_text()\n",
    "    ratings.append(rate)\n",
    "    \n",
    "    for i in range(9):\n",
    "        \n",
    "        common=c[1].find_all('tr',class_='table-body')\n",
    "        name=common[i].find('a').get_text()\n",
    "        names.append(name)\n",
    "        team=common[i].find_all('td')[2].get_text().strip()\n",
    "        teams.append(team)\n",
    "        rate=int(common[i].find_all('td')[3].get_text())\n",
    "        ratings.append(rate)\n",
    "    return (names,teams,ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d13de34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Trent Boult',\n",
       "  'Josh Hazlewood',\n",
       "  'Mitchell Starc',\n",
       "  'Shaheen Afridi',\n",
       "  'Matt Henry',\n",
       "  'Adam Zampa',\n",
       "  'Mehedi Hasan',\n",
       "  'Mujeeb Ur Rahman',\n",
       "  'Mustafizur Rahman',\n",
       "  'Rashid Khan'],\n",
       " ['NZ', 'AUS', 'AUS', 'PAK', 'NZ', 'AUS', 'BAN', 'AFG', 'BAN', 'AFG'],\n",
       " ['760', 727, 665, 661, 656, 655, 655, 650, 640, 635])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Soup_cricket_top10_bowlers('https://www.icc-cricket.com/rankings/mens/player-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7cffd6",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f55fd08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6a)\n",
    "\n",
    "def Soup_cricket_top10_teams_women(url):\n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    teams=[]\n",
    "    matches=[]\n",
    "    points=[]\n",
    "    ratings=[]\n",
    "    \n",
    "    c=soup.find('tr', class_='rankings-block__banner')\n",
    "    team=c.find_all('td')[1].find('span',class_='u-hide-phablet').get_text()\n",
    "    teams.append(team)\n",
    "    match=int(c.find_all('td')[2].get_text())\n",
    "    matches.append(match)\n",
    "    point=c.find_all('td')[3].get_text()\n",
    "    points.append(point)\n",
    "    rate=int(c.find_all('td')[4].get_text().strip())\n",
    "    ratings.append(rate)\n",
    "    \n",
    "    for i in range(9):\n",
    "        \n",
    "        common=soup.find_all('tr',class_='table-body')\n",
    "        team=common[i].find('span', class_='u-hide-phablet').get_text()\n",
    "        teams.append(team)\n",
    "        match=int(common[i].find_all('td')[2].get_text())\n",
    "        matches.append(match)\n",
    "        point=common[i].find_all('td')[3].get_text()\n",
    "        points.append(point)\n",
    "        rate=int(common[i].find_all('td')[4].get_text())\n",
    "        ratings.append(rate)\n",
    "    return (teams,matches,points,ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "279b5875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Australia',\n",
       "  'South Africa',\n",
       "  'England',\n",
       "  'India',\n",
       "  'New Zealand',\n",
       "  'West Indies',\n",
       "  'Bangladesh',\n",
       "  'Thailand',\n",
       "  'Pakistan',\n",
       "  'Sri Lanka'],\n",
       " [18, 26, 25, 27, 24, 24, 12, 8, 24, 8],\n",
       " ['3,061',\n",
       "  '3,098',\n",
       "  '2,904',\n",
       "  '2,820',\n",
       "  '2,425',\n",
       "  '2,334',\n",
       "  '932',\n",
       "  '572',\n",
       "  '1,519',\n",
       "  '353'],\n",
       " [170, 119, 116, 104, 101, 97, 78, 72, 63, 44])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Soup_cricket_top10_teams_women('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf870276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b)\n",
    "\n",
    "def Soup_cricket_top10_batting_women(url):\n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    names=[]\n",
    "    teams=[]\n",
    "    ratings=[]\n",
    "    \n",
    "    c=soup.find('tr', class_='rankings-block__banner')\n",
    "    name=c.find('div',class_='rankings-block__banner--name-large').get_text()\n",
    "    names.append(name)\n",
    "    team=c.find('div' ,class_='rankings-block__banner--nationality').get_text().strip()\n",
    "    teams.append(team)\n",
    "    rate=c.find('div', class_='rankings-block__banner--rating').get_text()\n",
    "    ratings.append(rate)\n",
    "    \n",
    "    for i in range(9):\n",
    "        \n",
    "        common=soup.find_all('tr',class_='table-body')\n",
    "        name=common[i].find('a').get_text()\n",
    "        names.append(name)\n",
    "        team=common[i].find_all('td')[2].get_text().strip()\n",
    "        teams.append(team)\n",
    "        rate=int(common[i].find_all('td')[3].get_text())\n",
    "        ratings.append(rate)\n",
    "    return (names,teams,ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "156997b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Sophie Ecclestone',\n",
       "  'Jess Jonassen',\n",
       "  'Megan Schutt',\n",
       "  'Shabnim Ismail',\n",
       "  'Jhulan Goswami',\n",
       "  'Hayley Matthews',\n",
       "  'Kate Cross',\n",
       "  'Ayabonga Khaka',\n",
       "  'Rajeshwari Gayakwad',\n",
       "  'Marizanne Kapp'],\n",
       " ['ENG', 'AUS', 'AUS', 'SA', 'IND', 'WI', 'ENG', 'SA', 'IND', 'SA'],\n",
       " ['739', 725, 722, 722, 698, 671, 657, 634, 617, 598])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Soup_cricket_top10_batting_women('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29ae4e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6c)\n",
    "\n",
    "def Soup_cricket_top10_Allrounders_women(url):\n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    names=[]\n",
    "    teams=[]\n",
    "    ratings=[]\n",
    "    \n",
    "    c=soup.find_all('div',class_='col-4 col-12-desk uniform-grid__section touch-scroll-list__element')\n",
    "    name=c[2].find('div',class_='rankings-block__banner--name').get_text()\n",
    "    names.append(name)\n",
    "    team=c[2].find('div' ,class_='rankings-block__banner--nationality').get_text().split('\\n')[2]\n",
    "    teams.append(team)\n",
    "    rate=c[2].find('div', class_='rankings-block__banner--rating').get_text()\n",
    "    ratings.append(rate)\n",
    "    \n",
    "    for i in range(9):\n",
    "        \n",
    "        common=c[2].find_all('tr',class_='table-body')\n",
    "        name=common[i].find('a').get_text()\n",
    "        names.append(name)\n",
    "        team=common[i].find_all('td')[2].get_text().strip()\n",
    "        teams.append(team)\n",
    "        rate=int(common[i].find_all('td')[3].get_text())\n",
    "        ratings.append(rate)\n",
    "    return (names,teams,ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be26a58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Hayley Matthews',\n",
       "  'Ellyse Perry',\n",
       "  'Natalie Sciver',\n",
       "  'Amelia Kerr',\n",
       "  'Marizanne Kapp',\n",
       "  'Deepti Sharma',\n",
       "  'Ashleigh Gardner',\n",
       "  'Jess Jonassen',\n",
       "  'Jhulan Goswami',\n",
       "  'Katherine Brunt'],\n",
       " ['WI', 'AUS', 'ENG', 'NZ', 'SA', 'IND', 'AUS', 'AUS', 'IND', 'ENG'],\n",
       " ['380', 374, 357, 356, 349, 322, 270, 246, 214, 207])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Soup_cricket_top10_Allrounders_women('https://www.icc-cricket.com/rankings/womens/player-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b39626",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b92d3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7)\n",
    "\n",
    "def cnbc_news(url):\n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    HeadLine=soup.find('h2',class_='FeaturedCard-packagedCardTitle').get_text()\n",
    "    NewsLink=soup.find('h2',class_='FeaturedCard-packagedCardTitle').find('a')['href']\n",
    "    p=requests.get(NewsLink)\n",
    "    s=BeautifulSoup(p.content)\n",
    "    Time=s.find('time').get_text()\n",
    "    return (HeadLine,NewsLink,Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f310b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Russian warship deployment puts Ukraine on high alert; Belarus says army must prepare for 'defense'\",\n",
       " 'https://www.cnbc.com/2022/12/01/russia-ukraine-live-updates.html',\n",
       " 'Updated Thu, Dec 1 20228:06 AM EST')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnbc_news('https://www.cnbc.com/world/?region=world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b690b343",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23f37339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8)\n",
    "\n",
    "def AI_articles(url):\n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    common=soup.find_all('li',class_='sc-9zxyh7-1 sc-9zxyh7-2 kOEIEO hvoVxs')\n",
    "    \n",
    "    Paper_Title=[]\n",
    "    Authors=[]\n",
    "    Published_Date=[]\n",
    "    Paper_URL=[]\n",
    "    \n",
    "    for i in range(len(common)):\n",
    "        \n",
    "        pt=common[i].find('a').get_text()\n",
    "        Paper_Title.append(pt)\n",
    "        a=common[i].find('span', class_='sc-1w3fpd7-0 dnCnAO').get_text()\n",
    "        Authors.append(a)\n",
    "        pd=common[i].find('span', class_='sc-1thf9ly-2 dvggWt').get_text()\n",
    "        Published_Date.append(pd)\n",
    "        pu=common[i].find('a')['href']\n",
    "        Paper_URL.append(pu)\n",
    "    return (Paper_Title,Authors,Published_Date,Paper_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1237106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Reward is enough',\n",
       "  'Making sense of raw input',\n",
       "  'Law and logic: A review from an argumentation perspective',\n",
       "  'Creativity and artificial intelligence',\n",
       "  'Artificial cognition for social humanârobot interaction: An implementation',\n",
       "  'Explanation in artificial intelligence: Insights from the social sciences',\n",
       "  'Making sense of sensory input',\n",
       "  'Conflict-based search for optimal multi-agent pathfinding',\n",
       "  'Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning',\n",
       "  'The Hanabi challenge: A new frontier for AI research',\n",
       "  'Evaluating XAI: A comparison of rule-based and example-based explanations',\n",
       "  'Argumentation in artificial intelligence',\n",
       "  'Algorithms for computing strategies in two-player simultaneous move games',\n",
       "  'Multiple object tracking: A literature review',\n",
       "  'Selection of relevant features and examples in machine learning',\n",
       "  'A survey of inverse reinforcement learning: Challenges, methods and progress',\n",
       "  'Explaining individual predictions when features are dependent: More accurate approximations to Shapley values',\n",
       "  'A review of possible effects of cognitive biases on interpretation of rule-based machine learning models',\n",
       "  'Integrating social power into the decision-making of cognitive agents',\n",
       "  \"âThat's (not) the output I expected!â On the role of end user expectations in creating explanations of AI systems\",\n",
       "  'Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies',\n",
       "  'Algorithm runtime prediction: Methods & evaluation',\n",
       "  'Wrappers for feature subset selection',\n",
       "  'Commonsense visual sensemaking for autonomous driving â On generalised neurosymbolic online abduction integrating vision and semantics',\n",
       "  'Quantum computation, quantum theory and AI'],\n",
       " ['Silver, David, Singh, Satinder, Precup, Doina, Sutton, Richard S. ',\n",
       "  'Evans, Richard, BoÅ¡njak, Matko and 5 more',\n",
       "  'Prakken, Henry, Sartor, Giovanni ',\n",
       "  'Boden, Margaret A. ',\n",
       "  'Lemaignan, SÃ©verin, Warnier, Mathieu and 3 more',\n",
       "  'Miller, Tim ',\n",
       "  'Evans, Richard, HernÃ¡ndez-Orallo, JosÃ© and 3 more',\n",
       "  'Sharon, Guni, Stern, Roni, Felner, Ariel, Sturtevant, Nathan R. ',\n",
       "  'Sutton, Richard S., Precup, Doina, Singh, Satinder ',\n",
       "  'Bard, Nolan, Foerster, Jakob N. and 13 more',\n",
       "  'van der Waa, Jasper, Nieuwburg, Elisabeth, Cremers, Anita, Neerincx, Mark ',\n",
       "  'Bench-Capon, T.J.M., Dunne, Paul E. ',\n",
       "  'BoÅ¡anskÃ½, Branislav, LisÃ½, Viliam and 3 more',\n",
       "  'Luo, Wenhan, Xing, Junliang and 4 more',\n",
       "  'Blum, Avrim L., Langley, Pat ',\n",
       "  'Arora, Saurabh, Doshi, Prashant ',\n",
       "  'Aas, Kjersti, Jullum, Martin, LÃ¸land, Anders ',\n",
       "  'Kliegr, TomÃ¡Å¡, BahnÃ­k, Å tÄpÃ¡n, FÃ¼rnkranz, Johannes ',\n",
       "  'Pereira, GonÃ§alo, Prada, Rui, Santos, Pedro A. ',\n",
       "  'Riveiro, Maria, Thill, Serge ',\n",
       "  'Kenny, Eoin M., Ford, Courtney, Quinn, Molly, Keane, Mark T. ',\n",
       "  'Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyton-Brown, Kevin ',\n",
       "  'Kohavi, Ron, John, George H. ',\n",
       "  'Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srikrishna ',\n",
       "  'Ying, Mingsheng '],\n",
       " ['October 2021',\n",
       "  'October 2021',\n",
       "  'October 2015',\n",
       "  'August 1998',\n",
       "  'June 2017',\n",
       "  'February 2019',\n",
       "  'April 2021',\n",
       "  'February 2015',\n",
       "  'August 1999',\n",
       "  'March 2020',\n",
       "  'February 2021',\n",
       "  'October 2007',\n",
       "  'August 2016',\n",
       "  'April 2021',\n",
       "  'December 1997',\n",
       "  'August 2021',\n",
       "  'September 2021',\n",
       "  'June 2021',\n",
       "  'December 2016',\n",
       "  'September 2021',\n",
       "  'May 2021',\n",
       "  'January 2014',\n",
       "  'December 1997',\n",
       "  'October 2021',\n",
       "  'February 2010'],\n",
       " ['https://www.sciencedirect.com/science/article/pii/S0004370221000862',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370221000722',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370215000910',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370298000551',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370216300790',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370218305988',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370220301855',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370214001386',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370299000521',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370219300116',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370220301533',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370207000793',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370216300285',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370220301958',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370297000635',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370221000515',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370221000539',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370221000096',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370216300868',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370221000588',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370221000102',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370213001082',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S000437029700043X',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370221000734',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0004370209001398'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_articles('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4837d4c9",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ebf98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9)\n",
    "\n",
    "def Dineout(url):\n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    common=soup.find_all('div',class_='restnt-main-wrap clearfix')\n",
    "    \n",
    "    Restaurant_name=[]\n",
    "    Cuisine=[]\n",
    "    Location=[]\n",
    "    Ratings=[]\n",
    "    Image_URL=[]    \n",
    "    \n",
    "    for i in range(len(common)):\n",
    "        \n",
    "        rn=common[i].find('div',class_='restnt-info cursor').find('a').get_text()\n",
    "        Restaurant_name.append(rn)\n",
    "        l=common[i].find('div',class_='restnt-loc ellipsis').get_text()\n",
    "        Location.append(l)\n",
    "        image=common[i].find('img',class_='no-img')['data-src']\n",
    "        Image_URL.append(image)\n",
    "        r=common[i].find('div',class_='restnt-rating rating-4').get_text()\n",
    "        Ratings.append(r)\n",
    "        c=common[i].find('span',class_='double-line-ellipsis').get_text().split('|')[1]\n",
    "        Cuisine.append(c)\n",
    "    return (Restaurant_name,Location,Image_URL,Ratings,Cuisine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f3ddd85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Castle Barbeque',\n",
       "  'Jungle Jamboree',\n",
       "  'Castle Barbeque',\n",
       "  'Cafe Knosh',\n",
       "  'The Barbeque Company',\n",
       "  'India Grill',\n",
       "  'Delhi Barbeque',\n",
       "  'The Monarch - Bar Be Que Village',\n",
       "  'Indian Grill Room'],\n",
       " ['Connaught Place, Central Delhi',\n",
       "  '3CS Mall,Lajpat Nagar - 3, South Delhi',\n",
       "  'Pacific Mall,Tagore Garden, West Delhi',\n",
       "  'The Leela Ambience Convention Hotel,Shahdara, East Delhi',\n",
       "  'Gardens Galleria,Sector 38A, Noida',\n",
       "  'Hilton Garden Inn,Saket, South Delhi',\n",
       "  'Taurus Sarovar Portico,Mahipalpur, South Delhi',\n",
       "  'Indirapuram Habitat Centre,Indirapuram, Ghaziabad',\n",
       "  'Suncity Business Tower,Golf Course Road, Gurgaon'],\n",
       " ['https://im1.dineout.co.in/images/uploads/restaurant/sharpen/8/k/b/p86792-16062953735fbe1f4d3fb7e.jpg?tr=tr:n-medium',\n",
       "  'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/p/m/p59633-166088382462ff137009010.jpg?tr=tr:n-medium',\n",
       "  'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/j/o/p38113-15959192065f1fcb666130c.jpg?tr=tr:n-medium',\n",
       "  'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/4/p/m/p406-15438184745c04ccea491bc.jpg?tr=tr:n-medium',\n",
       "  'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/7/p/k/p79307-16051787755fad1597f2bf9.jpg?tr=tr:n-medium',\n",
       "  'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/v/t/p2687-1482477169585cce712b90f.jpg?tr=tr:n-medium',\n",
       "  'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/d/i/p52501-1661855212630de5eceb6d2.jpg?tr=tr:n-medium',\n",
       "  'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/n/o/p34822-15599107305cfa594a13c24.jpg?tr=tr:n-medium',\n",
       "  'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/y/f/p549-165000147262590640c0afc.jpg?tr=tr:n-medium'],\n",
       " ['4.1', '3.9', '3.9', '4.3', '4', '3.9', '3.6', '3.8', '4.3'],\n",
       " [' Chinese, North Indian',\n",
       "  ' North Indian, Asian, Italian',\n",
       "  ' Chinese, North Indian',\n",
       "  ' Italian, Continental',\n",
       "  ' North Indian, Chinese',\n",
       "  ' North Indian, Italian',\n",
       "  ' North Indian',\n",
       "  ' North Indian',\n",
       "  ' North Indian, Mughlai'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dineout('https://www.dineout.co.in/delhi-restaurants/buffet-special?p=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130143a5",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de8b001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10)\n",
    "\n",
    "def Google_Scholar_Metrics(url):\n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    common=soup.find_all('tr')\n",
    "    \n",
    "    Rank=[]\n",
    "    Publication=[]\n",
    "    h5_index=[]\n",
    "    h5_median=[]\n",
    "\n",
    "    for i in range(1,len(common)):\n",
    "        \n",
    "        r=int(common[i].find('td',class_='gsc_mvt_p').get_text()[:-1])\n",
    "        Rank.append(r)\n",
    "        p=common[i].find('td',class_='gsc_mvt_t').get_text()\n",
    "        Publication.append(p)\n",
    "        index=int(common[i].find_all('td',class_='gsc_mvt_n')[0].get_text())\n",
    "        h5_index.append(index)\n",
    "        median=int(common[i].find_all('td',class_='gsc_mvt_n')[1].get_text())\n",
    "        h5_median.append(median)\n",
    "        \n",
    "    return (Rank,Publication,h5_index,h5_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9662486f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100],\n",
       " ['Nature',\n",
       "  'The New England Journal of Medicine',\n",
       "  'Science',\n",
       "  'IEEE/CVF Conference on Computer Vision and Pattern Recognition',\n",
       "  'The Lancet',\n",
       "  'Advanced Materials',\n",
       "  'Nature Communications',\n",
       "  'Cell',\n",
       "  'International Conference on Learning Representations',\n",
       "  'Neural Information Processing Systems',\n",
       "  'JAMA',\n",
       "  'Chemical Reviews',\n",
       "  'Proceedings of the National Academy of Sciences',\n",
       "  'Angewandte Chemie',\n",
       "  'Chemical Society Reviews',\n",
       "  'Journal of the American Chemical Society',\n",
       "  'IEEE/CVF International Conference on Computer Vision',\n",
       "  'Nucleic Acids Research',\n",
       "  'International Conference on Machine Learning',\n",
       "  'Nature Medicine',\n",
       "  'Renewable and Sustainable Energy Reviews',\n",
       "  'Science of The Total Environment',\n",
       "  'Advanced Energy Materials',\n",
       "  'Journal of Clinical Oncology',\n",
       "  'ACS Nano',\n",
       "  'Journal of Cleaner Production',\n",
       "  'Advanced Functional Materials',\n",
       "  'Physical Review Letters',\n",
       "  'Scientific Reports',\n",
       "  'The Lancet Oncology',\n",
       "  'Energy & Environmental Science',\n",
       "  'IEEE Access',\n",
       "  'PLoS ONE',\n",
       "  'Science Advances',\n",
       "  'Journal of the American College of Cardiology',\n",
       "  'Applied Catalysis B: Environmental',\n",
       "  'Nature Genetics',\n",
       "  'BMJ',\n",
       "  'Circulation',\n",
       "  'European Conference on Computer Vision',\n",
       "  'International Journal of Molecular Sciences',\n",
       "  'Nature Materials',\n",
       "  'Chemical engineering journal',\n",
       "  'AAAI Conference on Artificial Intelligence',\n",
       "  'Journal of Materials Chemistry A',\n",
       "  'ACS Applied Materials & Interfaces',\n",
       "  'Nature Biotechnology',\n",
       "  'The Lancet Infectious Diseases',\n",
       "  'Frontiers in Immunology',\n",
       "  'Applied Energy',\n",
       "  'Nano Energy',\n",
       "  'Nature Energy',\n",
       "  'Meeting of the Association for Computational Linguistics (ACL)',\n",
       "  'The Astrophysical Journal',\n",
       "  'Gastroenterology',\n",
       "  'Nature Methods',\n",
       "  'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'Cochrane Database of Systematic Reviews',\n",
       "  'Blood',\n",
       "  'Neuron',\n",
       "  'Nano Letters',\n",
       "  'Morbidity and Mortality Weekly Report',\n",
       "  'European Heart Journal',\n",
       "  'Nature Nanotechnology',\n",
       "  'ACS Catalysis',\n",
       "  'Nature Neuroscience',\n",
       "  'American Economic Review',\n",
       "  'Journal of High Energy Physics',\n",
       "  'IEEE Communications Surveys & Tutorials',\n",
       "  'Annals of Oncology',\n",
       "  'Nutrients',\n",
       "  'Accounts of Chemical Research',\n",
       "  'Immunity',\n",
       "  'Environmental Science & Technology',\n",
       "  'Nature Reviews. Molecular Cell Biology',\n",
       "  'Gut',\n",
       "  'Physical Review D',\n",
       "  'ACS Energy Letters',\n",
       "  'Monthly Notices of the Royal Astronomical Society',\n",
       "  'Conference on Empirical Methods in Natural Language Processing (EMNLP)',\n",
       "  'Clinical Infectious Diseases',\n",
       "  'Cell Metabolism',\n",
       "  'Nature Reviews Immunology',\n",
       "  'Joule',\n",
       "  'Nature Photonics',\n",
       "  'International Journal of Environmental Research and Public Health',\n",
       "  'Environmental Pollution',\n",
       "  'Computers in Human Behavior',\n",
       "  'Frontiers in Microbiology',\n",
       "  'Nature Physics',\n",
       "  'Small',\n",
       "  'Cell Reports',\n",
       "  'Molecular Cell',\n",
       "  'Clinical Cancer Research',\n",
       "  'Bioresource Technology',\n",
       "  'Journal of Business Research',\n",
       "  'Molecular Cancer',\n",
       "  'Sensors',\n",
       "  'Nature Climate Change',\n",
       "  'IEEE Internet of Things Journal'],\n",
       " [444,\n",
       "  432,\n",
       "  401,\n",
       "  389,\n",
       "  354,\n",
       "  312,\n",
       "  307,\n",
       "  300,\n",
       "  286,\n",
       "  278,\n",
       "  267,\n",
       "  265,\n",
       "  256,\n",
       "  245,\n",
       "  244,\n",
       "  242,\n",
       "  239,\n",
       "  238,\n",
       "  237,\n",
       "  235,\n",
       "  227,\n",
       "  225,\n",
       "  220,\n",
       "  213,\n",
       "  211,\n",
       "  211,\n",
       "  210,\n",
       "  207,\n",
       "  206,\n",
       "  202,\n",
       "  202,\n",
       "  200,\n",
       "  198,\n",
       "  197,\n",
       "  195,\n",
       "  192,\n",
       "  191,\n",
       "  190,\n",
       "  189,\n",
       "  186,\n",
       "  183,\n",
       "  181,\n",
       "  181,\n",
       "  180,\n",
       "  178,\n",
       "  177,\n",
       "  175,\n",
       "  173,\n",
       "  173,\n",
       "  173,\n",
       "  172,\n",
       "  170,\n",
       "  169,\n",
       "  167,\n",
       "  166,\n",
       "  165,\n",
       "  165,\n",
       "  165,\n",
       "  165,\n",
       "  164,\n",
       "  164,\n",
       "  163,\n",
       "  163,\n",
       "  163,\n",
       "  163,\n",
       "  162,\n",
       "  160,\n",
       "  160,\n",
       "  159,\n",
       "  159,\n",
       "  159,\n",
       "  159,\n",
       "  158,\n",
       "  158,\n",
       "  155,\n",
       "  155,\n",
       "  155,\n",
       "  155,\n",
       "  155,\n",
       "  154,\n",
       "  153,\n",
       "  153,\n",
       "  152,\n",
       "  152,\n",
       "  152,\n",
       "  152,\n",
       "  152,\n",
       "  152,\n",
       "  151,\n",
       "  151,\n",
       "  150,\n",
       "  149,\n",
       "  149,\n",
       "  146,\n",
       "  146,\n",
       "  145,\n",
       "  145,\n",
       "  145,\n",
       "  144,\n",
       "  144],\n",
       " [667,\n",
       "  780,\n",
       "  614,\n",
       "  627,\n",
       "  635,\n",
       "  418,\n",
       "  428,\n",
       "  505,\n",
       "  533,\n",
       "  436,\n",
       "  425,\n",
       "  444,\n",
       "  364,\n",
       "  332,\n",
       "  386,\n",
       "  344,\n",
       "  415,\n",
       "  550,\n",
       "  421,\n",
       "  389,\n",
       "  324,\n",
       "  311,\n",
       "  300,\n",
       "  315,\n",
       "  277,\n",
       "  273,\n",
       "  280,\n",
       "  294,\n",
       "  274,\n",
       "  329,\n",
       "  290,\n",
       "  303,\n",
       "  278,\n",
       "  294,\n",
       "  276,\n",
       "  246,\n",
       "  297,\n",
       "  307,\n",
       "  301,\n",
       "  321,\n",
       "  253,\n",
       "  265,\n",
       "  224,\n",
       "  296,\n",
       "  220,\n",
       "  223,\n",
       "  315,\n",
       "  296,\n",
       "  228,\n",
       "  217,\n",
       "  232,\n",
       "  314,\n",
       "  304,\n",
       "  234,\n",
       "  254,\n",
       "  296,\n",
       "  293,\n",
       "  243,\n",
       "  229,\n",
       "  231,\n",
       "  207,\n",
       "  302,\n",
       "  265,\n",
       "  264,\n",
       "  220,\n",
       "  248,\n",
       "  263,\n",
       "  220,\n",
       "  304,\n",
       "  243,\n",
       "  214,\n",
       "  211,\n",
       "  242,\n",
       "  214,\n",
       "  340,\n",
       "  235,\n",
       "  217,\n",
       "  212,\n",
       "  194,\n",
       "  249,\n",
       "  278,\n",
       "  211,\n",
       "  292,\n",
       "  233,\n",
       "  228,\n",
       "  225,\n",
       "  222,\n",
       "  214,\n",
       "  225,\n",
       "  222,\n",
       "  196,\n",
       "  205,\n",
       "  202,\n",
       "  201,\n",
       "  190,\n",
       "  233,\n",
       "  209,\n",
       "  201,\n",
       "  228,\n",
       "  212])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Google_Scholar_Metrics('https://scholar.google.com/citations?view_op=top_venues&hl=en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39d6211",
   "metadata": {},
   "source": [
    "### Done with Pleasure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
